"""
åŸºäºOpenAI Whisperçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)æœåŠ¡
ä¸“æ³¨äºæœ¬åœ°Whisperéƒ¨ç½²ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··æ‚è¯­éŸ³è¯†åˆ«
"""

import logging
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional

# æŠ‘åˆ¶ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
logging.getLogger("transformers").setLevel(logging.ERROR)

try:
    import torch
    import whisper
    WHISPER_AVAILABLE = True
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    CUDA_AVAILABLE = torch.cuda.is_available()
    if CUDA_AVAILABLE:
        print(f"ğŸš€ æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
    else:
        print("ğŸ’» ä½¿ç”¨CPUè¿›è¡ŒWhisperæ¨ç†")
except ImportError:
    WHISPER_AVAILABLE = False
    CUDA_AVAILABLE = False
    print("âš ï¸ Whisperæœªå®‰è£…ã€‚è¯·è¿è¡Œ: uv sync å®‰è£…ä¾èµ–")


class WhisperASR:
    """åŸºäºOpenAI Whisperçš„è¯­éŸ³è¯†åˆ«æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–Whisper ASRæœåŠ¡"""
        self.model = None
        self.current_model_size = None
        
    def transcribe_audio(self, audio_path: str, 
                        model_size: str = "base",
                        language: str = "auto",
                        **kwargs) -> Dict[str, Any]:
        """
        ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘æ–‡ä»¶ä¸ºæ–‡æœ¬
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            model_size: æ¨¡å‹å¤§å° ("tiny", "base", "small", "medium", "large")
            language: è¯­è¨€ä»£ç  ("auto", "zh", "en", "zh-en")
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            DictåŒ…å«è½¬å½•ç»“æœ
        """
        if not WHISPER_AVAILABLE:
            return {
                'success': False,
                'error': 'Whisperæœªå®‰è£…ã€‚è¯·è¿è¡Œ: uv sync',
                'text': '',
                'service': 'whisper'
            }
        
        audio_path = Path(audio_path)
        if not audio_path.exists():
            return {
                'success': False,
                'error': f'éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}',
                'text': '',
                'service': 'whisper'
            }
        
        print(f"ğŸ¤ å¼€å§‹ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘")
        print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        print(f"ğŸ§  æ¨¡å‹å¤§å°: {model_size}")
        
        try:
            # åŠ è½½æˆ–é‡ç”¨Whisperæ¨¡å‹
            if self.model is None or self.current_model_size != model_size:
                print(f"ğŸ“¥ åŠ è½½Whisperæ¨¡å‹: {model_size}")
                # æ ¹æ®GPUå¯ç”¨æ€§é€‰æ‹©è®¾å¤‡
                device = "cuda" if CUDA_AVAILABLE else "cpu"
                print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
                self.model = whisper.load_model(model_size, device=device)
                self.current_model_size = model_size
                print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({device})")
            
            # è®¾ç½®è¯­è¨€å‚æ•°
            whisper_language = self._get_whisper_language(language)
                
            # æ‰§è¡Œè½¬å½•
            print("ğŸ”„ æ­£åœ¨è½¬å½•éŸ³é¢‘...")
            result = self.model.transcribe(
                str(audio_path),
                language=whisper_language,
                verbose=False,
                **kwargs
            )
            
            text = result["text"].strip()
            detected_language = result.get("language", "unknown")
            segments = result.get("segments", [])
            
            # è¯­è¨€æ˜¾ç¤ºåç§°æ˜ å°„
            language_names = {
                'zh': 'ä¸­æ–‡',
                'en': 'è‹±æ–‡', 
                'ja': 'æ—¥æ–‡',
                'ko': 'éŸ©æ–‡',
                'unknown': 'æœªçŸ¥'
            }
            
            language_display = language_names.get(detected_language, detected_language)
            
            print(f"âœ… è½¬å½•å®Œæˆ!")
            print(f"ğŸŒ æ£€æµ‹è¯­è¨€: {language_display} ({detected_language})")
            print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} ä¸ªå­—ç¬¦")
            print(f"â±ï¸ åˆ†æ®µæ•°é‡: {len(segments)}")
            
            return {
                'success': True,
                'text': text,
                'language': detected_language,
                'language_display': language_display,
                'service': 'whisper',
                'model_size': model_size,
                'segments': segments,
                'segment_count': len(segments),
                'text_length': len(text),
                'message': f'æˆåŠŸä½¿ç”¨Whisper ({model_size}æ¨¡å‹) è½¬å½•éŸ³é¢‘'
            }
            
        except Exception as e:
            error_msg = f"Whisperè½¬å½•å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                'success': False,
                'error': error_msg,
                'text': '',
                'service': 'whisper',
                'model_size': model_size
            }
    
    def _get_whisper_language(self, language: str) -> Optional[str]:
        """
        è½¬æ¢è¯­è¨€å‚æ•°ä¸ºWhisperæ ¼å¼
        
        Args:
            language: è¾“å…¥è¯­è¨€è®¾ç½®
            
        Returns:
            Whisperè¯­è¨€ä»£ç æˆ–None
        """
        if language in ["auto", "zh-en"]:  # è‡ªåŠ¨æ£€æµ‹æˆ–ä¸­è‹±æ··æ‚
            return None
        elif language == "zh":
            return "zh"
        elif language == "en":
            return "en"
        else:
            return None  # è®©Whisperè‡ªåŠ¨æ£€æµ‹
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨çš„Whisperæ¨¡å‹åˆ—è¡¨
        
        Returns:
            å¯ç”¨æ¨¡å‹åˆ—è¡¨
        """
        if not WHISPER_AVAILABLE:
            return []
        
        return ["tiny", "base", "small", "medium", "large"]
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        models_info = {
            "tiny": {"size": "39MB", "speed": "æœ€å¿«", "accuracy": "è¾ƒä½", "description": "é€‚åˆå¿«é€Ÿæµ‹è¯•"},
            "base": {"size": "74MB", "speed": "å¿«", "accuracy": "è‰¯å¥½", "description": "å¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®ç‡ï¼Œæ¨è"},
            "small": {"size": "244MB", "speed": "ä¸­ç­‰", "accuracy": "è¾ƒå¥½", "description": "é€‚åˆä¸€èˆ¬ä½¿ç”¨"},
            "medium": {"size": "769MB", "speed": "è¾ƒæ…¢", "accuracy": "å¾ˆå¥½", "description": "é«˜å‡†ç¡®ç‡éœ€æ±‚"},
            "large": {"size": "1550MB", "speed": "æœ€æ…¢", "accuracy": "æœ€ä½³", "description": "æœ€é«˜å‡†ç¡®ç‡"}
        }
        
        return {
            "whisper_available": WHISPER_AVAILABLE,
            "cuda_available": CUDA_AVAILABLE,
            "current_model": self.current_model_size,
            "available_models": self.get_available_models(),
            "models_info": models_info,
            "supported_languages": ["auto", "zh", "en", "zh-en"],
            "recommended_model": "base",
            "device": "cuda" if CUDA_AVAILABLE else "cpu"
        }


def transcribe_audio_file(audio_path: str, model_size: str = "base", 
                         language: str = "auto", **kwargs) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘æ–‡ä»¶
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        model_size: æ¨¡å‹å¤§å°
        language: è¯­è¨€è®¾ç½®
        **kwargs: é¢å¤–å‚æ•°
        
    Returns:
        è½¬å½•ç»“æœ
    """
    asr = WhisperASR()
    return asr.transcribe_audio(audio_path, model_size, language, **kwargs)


def get_whisper_info() -> Dict[str, Any]:
    """
    è·å–WhisperæœåŠ¡ä¿¡æ¯
    
    Returns:
        æœåŠ¡ä¿¡æ¯
    """
    asr = WhisperASR()
    return asr.get_model_info()


if __name__ == "__main__":
    # æ˜¾ç¤ºWhisperä¿¡æ¯
    print("ğŸ¤ Whisper ASR æœåŠ¡ä¿¡æ¯:")
    info = get_whisper_info()
    
    print(f"âœ… Whisperå¯ç”¨: {info['whisper_available']}")
    print(f"ğŸš€ CUDAåŠ é€Ÿ: {info.get('cuda_available', False)}")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {info.get('device', 'unknown')}")
    print(f"ğŸ§  å¯ç”¨æ¨¡å‹: {', '.join(info['available_models'])}")
    print(f"â­ æ¨èæ¨¡å‹: {info['recommended_model']}")
    print(f"ğŸŒ æ”¯æŒè¯­è¨€: {', '.join(info['supported_languages'])}")
    
    # æ¨¡å‹è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ“Š æ¨¡å‹è¯¦ç»†ä¿¡æ¯:")
    for model, details in info['models_info'].items():
        print(f"  {model}: {details['size']}, {details['speed']}, {details['description']}")
    
    # å¦‚æœæœ‰æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ï¼Œè¿›è¡Œè½¬å½•æµ‹è¯•
    test_audio_dir = Path("downloads")
    if test_audio_dir.exists():
        audio_files = list(test_audio_dir.glob("*.webm"))
        if audio_files and info['whisper_available']:
            print(f"\nğŸ§ª æµ‹è¯•è½¬å½•ç¬¬ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶...")
            test_file = audio_files[0]
            print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
            
            result = transcribe_audio_file(str(test_file), "base", "auto")
            
            if result['success']:
                print(f"âœ… è½¬å½•æˆåŠŸ!")
                print(f"ğŸŒ æ£€æµ‹è¯­è¨€: {result['language_display']}")
                print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {result['text_length']}")
                print(f"ğŸ“„ æ–‡æœ¬é¢„è§ˆ: {result['text'][:150]}...")
            else:
                print(f"âŒ è½¬å½•å¤±è´¥: {result['error']}")
        elif not info['whisper_available']:
            print(f"\nâš ï¸ Whisperæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œè½¬å½•æµ‹è¯•")
        else:
            print(f"\nğŸ“‚ æœªæ‰¾åˆ°æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ ({test_audio_dir})")
    
    print(f"\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print(f"  - å¯¹äºä¸­è‹±æ–‡æ··æ‚çš„YouTubeè§†é¢‘ï¼Œæ¨èä½¿ç”¨'auto'è¯­è¨€æ£€æµ‹")
    print(f"  - 'base'æ¨¡å‹æä¾›æœ€ä½³çš„é€Ÿåº¦å’Œå‡†ç¡®ç‡å¹³è¡¡")
    print(f"  - å¦‚éœ€æœ€é«˜å‡†ç¡®ç‡ï¼Œä½¿ç”¨'large'æ¨¡å‹ï¼ˆä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰")
